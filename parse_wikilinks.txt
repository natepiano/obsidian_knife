Requirements:

Invalid Wikilink Detection:

Detect improperly nested brackets (e.g., [[Cory Sandahl|Cory]] Sandahl|Cory]])
Detect nested opening brackets (e.g., [[Cory Sandahl[[Cory Sandahl|Cory]])
Detect double aliases (e.g., [[wine|[[wine|white wine]]]])
Detect any other malformed wikilink structures
Handle both escaped and unescaped pipes in markdown tables
Track the exact position (span) of each invalid wikilink

please note how obsidian parses links

[[wikilink\\]] here]]

displays "wikilink\\" as a valid link and then here]] is just rendered as normal text
in our processing [[wikilink\\]] is not technically wrong but the ending here]] is
we would consider that an unmatched ending

this
[[blah [[blah]] blah ]]

would render a wikilink display as "blah [[blah" - with extra text of blah ]]

in this case we would consider the "blah [[blah" as having a nested opening wikilink
and the blah ]] as an unmatched wikilink

this
[[blah
woud be considered an unmatched opening


File-Level Tracking:

Invalid wikilinks must be tracked per file, not globally
Store invalid wikilinks in MarkdownFileInfo structure
Each invalid wikilink should maintain its context (line number, content)


Performance Requirements:

Avoid using regular expressions for detection
Maintain current performance characteristics
Use efficient state machine parsing approach


Integration with Back Population:

Use invalid wikilink spans as exclusion zones
Don't attempt to replace text within invalid wikilink spans
Present invalid wikilinks in a separate table in output
Handle both table and non-table contexts correctly


Backward Compatibility:

Keep existing collect_all_wikilinks return signature initially
Maintain existing test coverage
Ensure changes don't break existing functionality

Error Handling:

Provide clear error messages for each type of invalid wikilink
Maintain current error propagation patterns
Add appropriate error context

Implementation Constraints:

Make minimal, incremental changes
Keep all tests passing at each step
Modify existing tests rather than creating new ones when possible
Use TempDir for all file operation tests
Keep changes localized until necessary to expand

Test requirements:
prefer modifying existing tests over creating new ones unless you have to
organize tests under wikilink_creation module

Proposed Step-by-Step Plan:
[Previous plan remains valid, now supported by more detailed requirements]

Update parse_wikilink Implementation
a. Enhance State enum for better tracking
b. Add span tracking
c. Implement invalid pattern detection
d. Update return type
e. Update existing tests
f. Add specific invalid case tests

Update collect_all_wikilinks

Keep existing return signature (HashSet<CompiledWikilink>)
Add parameter to collect invalid wikilinks
Update logic to handle both valid and invalid results
Update existing collect_all_wikilinks tests
Add tests for invalid wikilink collection

Modify MarkdownFileInfo

Add invalid_wikilinks field to store InvalidWikilink instances
Update constructor and related functions
Update existing MarkdownFileInfo tests

Update scan_markdown_file

Modify to store invalid wikilinks in MarkdownFileInfo
Update tests for scan_markdown_file to verify invalid wikilink collection


Enhance back_populate Processing

Add logic to use invalid wikilink spans as exclusion zones
Create function to format invalid wikilinks table
Update existing back_populate tests
Add tests for exclusion zone functionality
